---
title: "BreastCancer Wisconsin Diagnostic dataset"
author: "Shravan Kuchkula"
date: "10/22/2017"
output:
  github_document:
    toc: yes
  html_document:
    keep_md: yes
    theme: cosmo
    toc: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
---

## Introduction
Features are computed from a digitized image of a fine needle	aspirate (FNA) of a breast mass.  They describe	characteristics of the cell nuclei present in the image.

## Data set
Breast Cancer Wisconsin data set from the [*UCI Machine learning repo*](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) is used to conduct the analysis. 

## Importing and Cleaning the data

Before importing, let's first load the required libraries.

```{r message=FALSE, warning=FALSE}
source('libraries.R')
```

Using read.csv we can download the dataset as shown:

```{r}
#url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
#download.file(url, 'wdbc.csv')

# use read_csv to the read into a dataframe
# columnNames are missing in the above link, so we need to give them manually.
columnNames <- c("id","diagnosis","radius_mean","texture_mean","perimeter_mean",
                 "area_mean","smoothness_mean","compactness_mean","concavity_mean",
                 "concave_points_mean","symmetry_mean","fractal_dimension_mean",
                 "radius_se","texture_se","perimeter_se","area_se","smoothness_se",
                 "compactness_se","concavity_se","concave_points_se","symmetry_se",
                 "fractal_dimension_se","radius_worst","texture_worst","perimeter_worst",
                 "area_worst","smoothness_worst","compactness_worst","concavity_worst",
                 "concave_points_worst","symmetry_worst","fractal_dimension_worst")
#wdbc <- read_csv(url, col_names = columnNames, col_types = NULL)
wdbc <- read.csv('wdbc.csv', header = FALSE, col.names = columnNames)
```

Let's take a peak
```{r}
glimpse(wdbc)
```

## Exploratory Data Analysis

Our response variable is diagnosis: Benign (B) or Malignant (M).
We have 3 sets of 10 numeric variables: mean, se, worst

Let's first collect all the 30 numeric variables into a matrix

```{r}
# Convert the features of the data: wdbc.data
wdbc.data <- as.matrix(wdbc[,c(3:32)])

# Set the row names of wdbc.data
row.names(wdbc.data) <- wdbc$id

# Create diagnosis vector
diagnosis <- as.numeric(wdbc$diagnosis == "M")
```

Let's answer some basic questions:

### How many observations are in this dataset ?

```{r}
nrow(wdbc.data)
```

### How many variables/features in the data are suffixed with _mean, _se, _worst?

```{r}
sum(endsWith(colnames(wdbc.data), "_mean"))
sum(endsWith(colnames(wdbc.data), "_se"))
sum(endsWith(colnames(wdbc.data), "_worst"))
```

### How many observations have benign or malignant diagnosis ?

```{r}
table(wdbc$diagnosis)
```
 
### What is the mean of each of the numeric columns ?

```{r}
round(colMeans(wdbc.data),2)
```

### What is the sd of each of the numeric columns ?

```{r}
roundSD <- function(x){
  round(sd(x),2)
}
apply(wdbc.data, 2, roundSD)
```

### How are the variables related to each other ?

```{r}
corMatrix <- wdbc[,c(3:32)]


# Rename the colnames
cNames <- c("rad_m","txt_m","per_m",
                 "are_m","smt_m","cmp_m","con_m",
                 "ccp_m","sym_m","frd_m",
                 "rad_se","txt_se","per_se","are_se","smt_se",
                 "cmp_se","con_se","ccp_se","sym_se",
                 "frd_se","rad_w","txt_w","per_w",
                 "are_w","smt_w","cmp_w","con_w",
                 "ccp_w","sym_w","frd_w")

colnames(corMatrix) <- cNames

# Create the correlation matrix
M <- round(cor(corMatrix), 2)

# Create corrplot
corrplot(M, diag = FALSE, method="color", order="FPC", tl.srt = 90, cl.offset = 0.5)

```

From the corrplot, it is evident that there are many variables that are highly correlated with each other. 

## PCA

Due to the number of variables in the model, we can try using a dimentionality reduction technique to unveil any patterns in the data. It also helps in visualizing a multi-dimentional dataset like this.

The first step in doing a PCA, is to ask ourselves whether or not the data should be scaled to unit variance. That is, to bring all the numeric variables to the same scale. In other words, we are trying to determine whether we should use a correlation matrix or a covariance matrix in our calculations of eigen values and eigen vectors (aka principal components).

Based on the output from `mean` and `sd`, it does appear that some variables have larger variance, e.g. area_worst. Hence we should use scaling. (will get this later and prove why we need to scale)

### Running a PCA using covariance matrix:

```{r}
wdbc.pcov <- princomp(wdbc.data, scores = TRUE)
summary(wdbc.pcov)
```

Get the eigen values:

```{r}
# Eigen values using covariance matrix
round(wdbc.pcov$sdev ^2,4)
```


Bi-plot using covariance matrix:
```{r warning=FALSE}
cex.before <- par("cex")
par(cex = 0.7)
biplot(wdbc.pcov)
par(cex = cex.before)
```

Scree plots can be useful in deciding how many PC's we should keep in the model. Let's create the scree-plots in R. As there is no R function to create a scree-plot, we need to prepare the data for the plot.

```{r}
# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.cvar <- wdbc.pcov$sdev ^ 2

# Variance explained by each principal component: pve
pve_cov <- pr.cvar/sum(pr.cvar)
```

Create a plot of variance explained for each principal component.

Scree plot using covariance matrix:
```{r}
# Plot variance explained for each principal component
plot(pve_cov, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve_cov), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```


Running PCA using correlation matrix:

```{r}
wdbc.pr <- prcomp(wdbc.data, scale = TRUE, center = TRUE)
summary(wdbc.pr)
```

84.73% of the variation is explained by the first five PC's.

### Bi-Plot

Let's create a bi-plot to visualize this:
```{r}
cex.before <- par("cex")
par(cex = 0.7)
biplot(wdbc.pr)
par(cex = cex.before)
```

From the above bi-plot of PC1 vs PC2, we can see that all these variables are trending in the same direction and most of them are highly correlated (More on this .. we can visualize this in a corrplot)

Create a scatter plot of observations by components 1 and 2
```{r}
# Scatter plot observations by components 1 and 2
plot(wdbc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")
```

There is a clear seperation of diagnosis (M or B) that is evident in the PC1 vs PC2 plot.

Let's also take PC1 vs PC3 plot:
```{r}
# Repeat for components 1 and 3
plot(wdbc.pr$x[, c(1,3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.

### Scree plots

Scree plots can be useful in deciding how many PC's we should keep in the model. Let's create the scree-plots in R. As there is no R function to create a scree-plot, we need to prepare the data for the plot.

```{r}
# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- wdbc.pr$sdev ^ 2

# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)
```

Create a plot of variance explained for each principal component.

```{r}
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

Scree-plots suggest that 80% of the variation in the numeric data is captured in the first 5 PCs.

## LDA

As found in the PCA analysis, we can keep 5 PCs in the model. Our next task is to use the first 5 PCs to build a Linear discriminant function using the `lda()` function in R. 

From the `wdbc.pr` object, we need to extract the first five PC's. To do this, let's first check the variables available for this object.

```{r}
ls(wdbc.pr)
```

We are interested in the `rotation` (also called loadings) of the first five principal components multiplied by the scaled data, which are called `scores` (basically PC transformed data)

```{r}
wdbc.pcs <- wdbc.pr$x[,1:5]
head(wdbc.pcs, 20)
```

Here, the rownames help us see how the PC transformed data looks like.

Now, we need to append the `diagnosis` column to this PC transformed data frame `wdbc.pcs`. Let's call the new data frame as `wdbc.pcst`. 

```{r}
wdbc.pcst <- wdbc.pcs
wdbc.pcst <- cbind(wdbc.pcs, diagnosis)
head(wdbc.pcst)
```

Here, diagnosis == 1 represents malignant and diagnosis == 0 represents benign.

## Model Validation
To evaluate the effectiveness of our model in predicting the diagnosis of breast cancer, we can split the original data set into training and test data. Using the training data, we will build the model and predict using the test data. We will then compare the predictions with the original data to check the accuracy of our predictions. We will use three approaches to split and validate the data. In the first approach, we use 75% of the data as our training dataset and 25% as our test dataset. In the second approach, we use 3-fold cross validation and in the third approach we extend that to a 10-fold cross validation. 

### Splitting the dataset into training/test data

When creating the LDA model, we can split the data into training and test data. Using the training data we can build the LDA function. Next, we use the test data to make predictions. 

```{r}
# Calculate N
N <- nrow(wdbc.pcst)

# Create a random number vector
rvec <- runif(N)

# Select rows from the dataframe
wdbc.pcst.train <- wdbc.pcst[rvec < 0.75,]
wdbc.pcst.test <- wdbc.pcst[rvec >= 0.75,]

# Check the number of observations
nrow(wdbc.pcst.train)
nrow(wdbc.pcst.test)
```

So, `r nrow(wdbc.pcst.train)` observations are in training dataset and `r nrow(wdbc.pcst.test)` observations are in the test dataset. 
We will use the training dataset to calculate the `linear discriminant function` by passing it to the `lda()` function of the `MASS` package. 

```{r message=FALSE}
library(MASS)

wdbc.pcst.train.df <- wdbc.pcst.train

# convert matrix to a dataframe
wdbc.pcst.train.df <- as.data.frame(wdbc.pcst.train)

# Perform LDA on diagnosis
wdbc.lda <- lda(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = wdbc.pcst.train.df)
```

Let's summarize the LDA output:

```{r}
wdbc.lda
```

Let's use this to predict by passing the predict function's newdata as the testing dataset.

```{r}
wdbc.pcst.test.df <- wdbc.pcst.test

# convert matrix to a dataframe
wdbc.pcst.test.df <- as.data.frame(wdbc.pcst.test)

wdbc.lda.predict <- predict(wdbc.lda, newdata = wdbc.pcst.test.df)

```

Let's check what functions we can invoke on this predict object:
```{r}
ls(wdbc.lda.predict)
```

Our predictions are contained in the `class` attribute.

```{r}
# print the predictions
(wdbc.lda.predict.class <- wdbc.lda.predict$class)
```

Next, compare the accuracy of these predictions with the original data.

A simple way to validate the accuracy of our model in predicting diagnosis (M or B) is to compare the test data result to the observed data. Find the proportion of the errors in prediction and see whether our model is acceptable. 

```{r}
(confusionMat <- table(wdbc.lda.predict.class, wdbc.pcst.test.df$diagnosis))
```

So according to this output, the model predicted  `r confusionMat[1]` times that the diagnosis is 0 (benign) when the actual observation was 0 (benign) and `r confusionMat[3]` times it predicted incorrectly. Similarly, the model predicted that the diagnosis is 1 (malignant) `r confusionMat[4]` times correctly and `r confusionMat[2]` predicted incorrectly. 

The accuracy of this model in predicting benign tumors is `r confusionMat[1] / (confusionMat[1] + confusionMat[3])`  or `r (confusionMat[1] / (confusionMat[1] + confusionMat[3])) * 100`% accurate.
The accuracy of this model in predicting malignant tumors is `r confusionMat[4] / (confusionMat[2] + confusionMat[4])` or `r (confusionMat[4] / (confusionMat[2] + confusionMat[4])) * 100`% accurate.

What is the classification accuracy of this model ?

What is the sensitivity of this model ?

What is the Specificity of this model ?

> Note: The above table is termed as a confusion matrix. From a confusion matrix you can calculate some measures such as: classification accuracy, sensitivity and specificity.

### 3-fold cross validation


We can implement a cross-validation plan using the `vtreat` package's `kWayCrossValidation` function. 
Syntax: kWayCrossValidation(nRows, nSplits, dframe, y)

nRows   - number of rows in the training data
nSplits - number of folds (partitions) in the cross-validation. E.g, 3 for 3-way CV
remaining 2 arguments not needed.

The function returns indicies for training and test data for each fold. Use the data with the training indicies to fit the model and then make predictions using the test indicies. If the estimated model performance looks good, then use all the data to fit a final model. You can't evaluate this final model, becuase you don't have data to evaluate it with. Cross Validation only tests the modeling process, while the test/train split tests the final model. 

```{r}
library(vtreat)

# convert wdbc.pcst to a dataframe
wdbc.pcst.df <- as.data.frame(wdbc.pcst)

nRows <- nrow(wdbc.pcst.df)
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)

# examine the split plan
str(splitPlan)
```

Here, k is the number of folds and `splitplan` is the cross validation plan

```{r}
# Run a 3-fold cross validation plan from splitPlan
k <- 3

for ( i in 1:k ) {
  split <- splitPlan[[i]]
  model <- lda(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = wdbc.pcst.df[split$train,])
  model.pred.cv <- predict(model, newdata = wdbc.pcst.df[split$app,])
  
  confMat <- table(model.pred.cv$class, wdbc.pcst.df$diagnosis[split$app])
  print(confMat)
}



```

### 10-fold cross validation

Running a 10-fold cross validation:

```{r}
# Run a 10-fold cross validation plan from splitPlan
k <- 10
nRows <- nrow(wdbc.pcst.df)
splitPlan <- kWayCrossValidation(nRows, 10, NULL, NULL)

# Run a 10-fold cross validation plan from splitPlan

for ( i in 1:k ) {
  split <- splitPlan[[i]]
  model <- lda(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5, data = wdbc.pcst.df[split$train,])
  model.pred.cv <- predict(model, newdata = wdbc.pcst.df[split$app,])
  
  confMat <- table(model.pred.cv$class, wdbc.pcst.df$diagnosis[split$app])
  print(confMat)
}

```

An advanced way of validating the accuracy of our model is by using a k-fold cross-validation. 

When we split the data into training and test data set, we are essentially doing 1 out of sample test. However, this process is a little fragile. A better approach than a simple train/test split, using multiple test sets and averaging out of sample error - which gives us a more precise estimate of the true out of sample error. One of the most common approaches for multiple test sets is Cross Validation. 

## Conclusion










